## How to run:
1. Clone and cd into repo
2. Make sure you have the needed json files
3. pip install virtualenv
4. virtualenv env
5. source env/bin/activate
6. pip install ipython
7. ipython notebook

Note: steps 3-5 are optional... they are creating a virtual environment

Note: any code that is documented in the MVP.ipynb is taken from a library of functions I have written

## The idea:
```
I treated this challenge as a classification problem, associate every publication with the most 
likely author, where each author can be considered as a label, and then you mine the publication 
metadata for features for the classification task. I did assume that as there were examples of 
publications without authors that the overall point was to be able to assign authors to the publications.

My idea was to create an ensemble classifier of Naive Bayes and Random Forest in order to correctly 
assign a document to an author. Naive Bayes classifiers when used with features generated by character
n-grams have shown to have nearly state of the art performance in classifying text. However, due to the
limited amount of text available per publication, the great number of labels (each author is a
potential label), and the fact that inherently this set up is a bit flawed due to the fact that there will
be multiple entries where the feature set of n-grams will be the same while the labels will be different, 
(multiple authors per paper), I didn't want to fully trust the results of the Naive Bayes classifier. 

(Just as an FYI, I was also contemplating only using the leading author as the label, so that
the problem became identify the leading author of each publication, as that would mean less confusion from
the classifier's perspective.)

Instead, as long as the Naive Bayes classifier was doing better than random, I would feed
the most likely label (author_id) that came out of the classifier into another classifier along with other
features. The second classifier I would have chosen would have been a Random Forest classifier, which
has shown to be a robust and useful multiclass classification tool. Feeding the result of the Naive Bayes
classifier, along with:

  1. vector representation (bow style) of the journal name (restricted to about 30 terms over all journals)
  2. language of journal (using something like langid.py)
  3. year of publication
  4. number of authors
  5. length of title
  6. length of abstract
  7. number of punctuation characters in abstract
  8. number of numeric characters in abstract
  
into the Random Forest classifier I would have used the label that came out of this second classifier as the
final predicted label (author) of the publication.

The evaluation of this would be a little tricky due to the multiple authors per publication aspect, but if 
we were to restrict the problem to just associate the leading author to the paper, then we could use macro
and micro precision, recall, f-scores along with a confusion matrix to evaluate performance.
```

## What I got to:

  1. Reading in the data
  2. Creating a Publication class that will allow for me to easily store features in one nice structure
  3. Using various cleaning techniques to ensure some amount of data integrity (using my utility functions)
  4. Generating unigram, bigram, trigram, quadgram frequencies for each publication, where the text being considered
     was a concatenation of title and abstract, stripped for all non-alpha characters
  5. Keeping all overall frequency count of 1-4 grams for all publications, and pruning the potential feature space
     to only consider all grams who have a count greater than 5 (subject to param tuning), and who lie in the bottom
     half of counts when sorting by frequency, as the most common grams are probably very similar across all publications.
     The idea here is not use every possible gram as a dimension in the feature space for the Naive Bayes classifier, but
     only those grams that will provide the most information gain. Removing all grams with a count lower than a certain
     amount will remove some noise from the long tail, while removing the most popular grams will also remove a lot of the 
     common grams used across all publications, as those grams wouldn't help in discerning that this paper belongs to this 
     author.
 
## What is left
   1. Split data into train and test
   2. Create a dictionary where we used a combination of firstinitial, firstname,  middleinitial, middlename, lastname, 
      and email to hopefully create a somewhat unique string rep of each author to be used as the keys in the dictionary. The
      values in the dictionary would be the associated label with that author. Note all nulls are replaced by "".
   3. Create feature, label pairings for each publication, where the feature would be the frequencies of the grams that
      made it through the pruning described above, and the label would be the label associated to either just the leading
      author, or create multiple pairs of feature, label for each author.
   4. Train a NB multinomial classifier using this: 
      http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html
   5. Store the result for each Publication inside the Publication object
   6. Create functions that would create the other features to be used in the Random Forest Classification (most of them
      are counts)
   7. Create feature, label pairs for the Random Forest Classification
   8. Train a Random Forest Classifier using this:
      http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
   9. Evaluate performance.
